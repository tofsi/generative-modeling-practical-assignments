{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux6XpaF_qFlt"
      },
      "source": [
        "# **Variational Auto-Encoders (VAEs) and the Evidence Lower-BOund (ELBO)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_WhtXhorWnX"
      },
      "source": [
        "### This practical\n",
        "\n",
        "\n",
        "\n",
        "*   provides skeleton code for a VAE – you are tasked with filling in the missing bits and pieces\n",
        "*   has a couple of experiments to visualize interesting/useful aspects of the VAE \n",
        "*   is implemented using the python library [JAX](https://jax.readthedocs.io/en/latest/index.html)\n",
        "*   will give a conceptual, rather than technical, introduction to the ELBO and variants thereof\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwfdj2Q7y4FI"
      },
      "source": [
        "### Before getting started, let us recap the VAE. \n",
        "\n",
        "#### Background\n",
        "\n",
        "VAE was simultaneously proposed by [Diederik P. Kingma and Max Welling](https://arxiv.org/abs/1312.6114) [1] and [Danilo Jimenez Rezende, Shakir Mohamed and Daan Wierstra](https://arxiv.org/abs/1401.4082) [2] independently. The former was published slightly prior to the latter, and has recieved considerably more citations. If you have not yet read/looked at either, I recommend checking out [1], as it will help you in this practical (and, of course, since it is a great piece of work). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nazeKz6FhRk"
      },
      "source": [
        "Essentially, a VAE can be seen as a mapping from an input space to a latent (hidden or unobserved) space, then from the latent space to an output space. It consists of two parts, called encoder and decoder. With the help of encoder, we can map a given input sample $x_{ori} \\sim p(x)$ to a distribution over the latent space. The distribution is denoted by $q_{\\phi}(z|x_{ori})$ and $\\phi$ denotes the parameters of the encoder (optimized via training), called ([variational](https://www.cs.jhu.edu/~jason/tutorials/variational.html)) parameters. \n",
        "\n",
        "In mathematics, finding the best function (the distribution over the latent space in our case, i.e. $q_{\\phi}(z|x_{ori})$) is done under the name of calculus of variations and this name also comes from that mathematical background. Variational approximation, (finding the best or closest function to a given function, in our case the true distribution over the latent sapce $p(z|x_{ori})$) is usually done because of the complexity of the original function (it means, it is hard to work with $p(z|x_{ori})$ because it has interactable terms)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwOYAogQl5W4"
      },
      "source": [
        "So the idea of variational approximation is that to fix a family of distributions, say $Q$ (think of it as the set of all possible normal distributions $Q=\\{\\mathcal{N}(\\mu,\\sigma)| \\mu \\in \\mathbb{R}, \\sigma \\in \\mathbb{R}^+\\}$), and then approximate a complex function, needless to say $p(z|x_{ori})$, with a simpler function $q_\\phi(z|x_{ori}) \\in Q$. To this end, we need to find the closest $q_\\phi(z|x_{ori})$ to $p(z|x_{ori})$, and the parameters of $q_\\phi(z|x_{ori})$ (in our example, a good $\\mu$ and $\\sigma$) are called **variational parameters**. The subscript $\\phi$, also called variational paramters or encoder paramters, denotes that the distribution we get, depends on some other variables that has to be optimized (i.e. the variables that we use to map a given input $x$ to a distribution over the latent space, for example $\\phi=\\{W_1,W_2,b_1,b_2\\}$ if we are using a two layer MLP)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Y2dwNzmnx4"
      },
      "source": [
        "Note that $q_\\phi(z|x)$ is a function of two variables both $x$ and $z$ and when you plug in some specific $x$, you still have a function of $z$.  Therefore, $q_\\phi(z|x)$ gives us the paramters of a distribution (in our example it gives us some specific $\\mu$ and $\\sigma$ after plugging it $x$). Then, after getting the paramters of a distribution over the latent space $q_\\phi(z|x_{ori})$, it is time to reconstruct the input sample $x_{ori}$ from its latent representation $q_\\phi(z|x_{ori})$, but the problem is that we can not use a function as input into the decoder! So what we can do is to draw (several) samples from the latent distribution $q_\\phi(z|x_{ori})$ and recover a distribution over the  input space. Another the problem is that it is that, sampling is not differentiable, making our model indifferentiable. To see why, try to diffrentiate a sample drawn from a Normal distribution, i.e. while $x \\sim \\mathcal{N}(\\mu,\\sigma)$ try to compute the following: \n",
        "$$\n",
        "\\frac{dx}{d\\mu}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfonqDcWrNvD"
      },
      "source": [
        "To solve the problem regarding the sampling and indifferentiability, [1] have proposed a trick called **reparameterization trick**, of which you'll hear more in the rest of this practical. But for now, it suffices to assume that we know we can use a sample like $z_{ori} \\sim q_\\phi(z|x_{ori})$, to map it back to the input sapce again, using a decoder, or so called **the generative** model. In our example, it is as simple as $z_{ori} \\sim \\mathcal{N}(\\mu_{ori},\\sigma_{ori})$ where the $\\mu_{ori}$ and $\\sigma_{ori}$ is obtained simply by plugging in $x_{ori}$ into the encoder, i.e. $q_\\phi(z|x_{ori})$.\n",
        "\n",
        "As the decoder is acting on samples of a distribution, and becomes stochastic by virtue of sampling, it defines a distribution over the input space. Therefore, the **distribution recovered** from $z_{ori}$ by the decoder, is denoted by $p_\\theta(x|z_{ori})$.\n",
        "\n",
        "The $\\theta$ subscript is the parameters of the generative model, also called generative paramters are optimized jointly with $\\phi$. Finally, we can *decode* $z_{ori}$ by sampling from $p_{\\theta}(x|z_{ori})$. The decoding of $z_{ori}$, i.e. $x_{rec}\\sim p_{\\theta}(x|z_{ori})$, is commonly referred to as the *reconstruction*.\n",
        "\n",
        "Note that in the original paper you do not see any distinction between $x_{ori}$, $x_{rec}$ and $x$ as they are in the same space, but it is added here to make it easy to explain the idea! Following [1], we avoid the use of subscripts in the rest of the practical. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMuYXfV7FjMC"
      },
      "source": [
        "### Why?\n",
        "There are plenty of interesting tasks that can be tackled using VAEs. Some important examples include dimensionality reduction (encoding $x$ as $z$, such that dim($z$) < dim($x$)), noise filtering (a.k.a. reconstructing the input), generating synthetic data (sampling from $p_{\\theta}(x|z)$) and density estimation. Let us zoom in on the latter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or6B7SrcIyQH"
      },
      "source": [
        "Say we are interested in learning the posterior distribution $p(z|x)$ for whatever reason. In most interesting scenarios, however, we cannot learn it since it is intractable. The intractability of\n",
        "\n",
        "$$\n",
        "  p(z|x) = \\frac{p(z,x)}{p(x)}\n",
        "$$\n",
        "\n",
        "stems from not being able to compute $p(x) = \\int p(z, x) dz$ as it involves marginalizing over all $z$. Instead of learning $p(z|x)$ directly, we have to approximate it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoGLmIP8wFMV"
      },
      "source": [
        "### How?\n",
        "The VAE is a form of (amortized) variational inference (VI). In VI we approximate the intractable posterior using a simpler distribution, $q_{\\phi}(z|x)$, inferring $\\phi$ by estimating the marginal log-likelihood, $\\log p(x)$. Consequently, we actually estimate two densities.\n",
        "\n",
        "The inference is performed by *maximizing* the **E**vidence **L**ower **BO**ound (ELBO in short), $\\mathcal{L}$:\n",
        "\n",
        "$$\\mathbb{E}_{q_{\\phi}(z|x)}\\left[\\log\\frac{p(z, x)}{q_{\\phi}(z|x)}\\right] \\triangleq \\mathcal{L} = - D_{KL}(q_{\\phi}(z|x)||p(z)) +\\mathbb{E}_{q_{\\phi}(z|x)}\\left[\\log p_\\theta(x|z)\\right]$$\n",
        "\n",
        "where $D_{KL}(q_{\\phi}(z|x)||p(z))$ is the Kullback-Leibler (KL) divergence. The KL divergence is strictly non-negative, and zero if and only if $q_{\\phi}(z|x) = p(z)$, while the marginal log-likelihood, $\\log p(x)$, is a constant. It is also possible to write:\n",
        "\n",
        "$$\\log p(x) = D_{KL}(q_{\\phi}(z|x)||p(z|x)) + \\mathcal{L}$$\n",
        "The $D_{KL}(q_{\\phi}(z|x)||p(z|x))$ term is called **the variational gap** and is non-negative, so it can be seen that we have:\n",
        "$$\\log p(x) \\geq \\mathcal{L}$$\n",
        "\n",
        "Therefore, the ELBO is a lower bound on the marginal log-likelihood and maximizing $\\mathcal{L}$ indirectly minimizes $D_{KL}(q_{\\phi}(z|x)||p(z|x))$ (which we do not have), giving us a better approximation of the intractable posterior.  See [1] for the derivation.\n",
        "Note that the negated ELBO is often referred to as the negative log-likelihood (NLL)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F3tU8mEAvjp"
      },
      "source": [
        "In this practical, as in [1], we will assume that $q_\\phi(z|x) = \\mathcal{N}(z;\\mu,\\sigma^2)$, $p(z)=\\mathcal{N}(0,1)$ and $p_{\\theta}(x|z) = \\text{Bernoulli}(x;θ$). Hence we can evaluate all terms in the ELBO, where $p(z,x) = p_\\theta(x|z)p(z)$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvOdT64y0m26"
      },
      "source": [
        "### Benchmarking VAEs in Density Estimation Tasks\n",
        "This will be a recurring topic in this practical. Namely, you may train on whichever objective function you deem fitting, however when reporting your results in order to compare the performance of your algorithm, the evaluation of the **correct** objective is crucial. For instance, if you want to compare your VAE's NLL scores with others', then make sure the objective function is in fact a lower-bound on the marginal log-likelihood. More on that later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVFulEOty0s-"
      },
      "source": [
        "## VAE in JAX\n",
        "Now, let's import the necessary libraries. Feel free to add imports as you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvSlSM5bpy6o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, grad, lax, random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax.example_libraries import stax\n",
        "from jax.example_libraries.stax import Dense, FanOut, Relu, Softplus, Sigmoid\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "from torchvision.datasets import MNIST, FashionMNIST\n",
        "from functools import partial\n",
        "\n",
        "# IMPORTANT NOTE:\n",
        "# if you have got a NaN loss and/or have trouble debugging. Then, set\n",
        "# jax_disable_jit to True. This will help you print out the variables.\n",
        "jax.config.update('jax_disable_jit', False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m5VyoGFuztQ"
      },
      "source": [
        "For the sake of simplicity, we use the MNIST dataset in this practical.\n",
        "\n",
        "The next cell creates a PyTorch dataloader. Using either PyTorch or TensorFlow dataloaders seems to (currently) be the practice when working with JAX. The code has been slighlty altered after being copying from [here](https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/Neural_Network_and_Data_Loading.ipynb#scrollTo=94PjXZ8y3dVF)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s4eEPNl4C02"
      },
      "outputs": [],
      "source": [
        "def numpy_collate(batch):\n",
        "  if isinstance(batch[0], np.ndarray):\n",
        "    return np.stack(batch)\n",
        "  elif isinstance(batch[0], (tuple,list)):\n",
        "    transposed = zip(*batch)\n",
        "    return [numpy_collate(samples) for samples in transposed]\n",
        "  else:\n",
        "    return np.array(batch)\n",
        "\n",
        "class NumpyLoader(data.DataLoader):\n",
        "  def __init__(self, dataset, batch_size=1,\n",
        "                shuffle=False, sampler=None,\n",
        "                batch_sampler=None, num_workers=0,\n",
        "                pin_memory=False, drop_last=False,\n",
        "                timeout=0, worker_init_fn=None):\n",
        "    super(self.__class__, self).__init__(dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        sampler=sampler,\n",
        "        batch_sampler=batch_sampler,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=numpy_collate,\n",
        "        pin_memory=pin_memory, \n",
        "        drop_last=drop_last,\n",
        "        timeout=timeout,\n",
        "        worker_init_fn=worker_init_fn)\n",
        "\n",
        "class FlattenAndCast(object):\n",
        "  def __call__(self, pic):\n",
        "    return np.ravel(np.array(pic, dtype=jnp.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWtB5R89au8m"
      },
      "outputs": [],
      "source": [
        "batch_size = 248\n",
        "# Define our dataset, using torch datasets\n",
        "mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
        "training_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)\n",
        "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False)\n",
        "test_images = jnp.array(mnist_dataset_test.test_data.numpy().reshape(len(mnist_dataset_test.test_data), -1), dtype=jnp.float32)\n",
        "test_labels = jnp.array(mnist_dataset_test.test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-LG0H-6yHIv"
      },
      "source": [
        "### Initializing the Network and Optimizer\n",
        "\n",
        "Next we put together a simple VAE, using two-layered MLPs with 300 hidden units for both the encoder and decoder networks. Note that the two parallel ultimate layers in the encoder net output latent_dimensions-dimensional arrays. **Based on the respective ranges of these two outputs, which one is $\\mu$ and which is $\\sigma$?**\n",
        "\n",
        "Also, note that we apply **the sigmoid function on the outputs of the decoder**. Recall that $p_\\theta(x|z)$ models the paramters of a Bernoulli distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SKXbqHTEBZH"
      },
      "source": [
        "Initialize the networks and the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_62MiYUq_B8"
      },
      "outputs": [],
      "source": [
        "latent_dimensions = 2\n",
        "\n",
        "# encoder defs\n",
        "encoder_init, encode = stax.serial(\n",
        "    Dense(300), Relu,\n",
        "    Dense(300), Relu,\n",
        "    FanOut(2),\n",
        "    stax.parallel(Dense(latent_dimensions), stax.serial(Dense(latent_dimensions), Softplus)),\n",
        ")\n",
        "\n",
        "# decoder defs\n",
        "decoder_init, decode = stax.serial(\n",
        "    Dense(300), Relu,\n",
        "    Dense(300), Relu,\n",
        "    Dense(28 * 28), Sigmoid\n",
        ")\n",
        "\n",
        "\n",
        "# initialize networks and get params\n",
        "enc_init_key, dec_init_key = random.split(random.PRNGKey(100))\n",
        "_, encoder_params = encoder_init(enc_init_key, input_shape=(-1, 28 * 28))\n",
        "_, decoder_params = decoder_init(dec_init_key,\n",
        "                                 input_shape=\n",
        "                                 (-1, latent_dimensions))\n",
        "params = encoder_params, decoder_params\n",
        "\n",
        "# initialize optimizer\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size=0.001)\n",
        "opt_state = opt_init(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXzxAASZETlT"
      },
      "source": [
        "### Task 1: Reparameterization Trick\n",
        "See [1] to figure out what **reparameterization trick** is, and implement the reparameterization trick in the following function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SDQN-96voLn"
      },
      "outputs": [],
      "source": [
        "def sample(rng, mu, std):\n",
        "  # use the reparameterization trick\n",
        "  return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiELFLZgFOta"
      },
      "source": [
        "Continuing with the VAE pipeline. \"@jit\" is a JAX decoration which apparently heavily speeds up operations. Beware, however, that it does not support all operations, putting restrictions on the function it decorates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiiDWUo33E1p"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def forward(params, x, rng):\n",
        "    encoder_params, decoder_params = params\n",
        "    phi = encode(encoder_params, inputs=x)\n",
        "    z = sample(rng, mu=phi[0], std=phi[1])\n",
        "    bernoulli_theta = decode(decoder_params, z)\n",
        "    return phi, z, bernoulli_theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJWXlO6hGWsO"
      },
      "source": [
        "### Task 2: ELBO\n",
        "**Compute the ELBO in the following function.** Feel free to add or remove input arguments, except for params; params should be the first input argument (it's due to how JAX computes gradients). Also, since the optimizer *minimizes* the objective function, make sure to return the negated ELBO. Hint: recall the definition of the ELBO, note that it can be Monte Carlo approximated as follows\n",
        "\n",
        "$\\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)}\\left[ \\log\\frac{p_\\theta(x|z)p(z)}{q_\\phi(z|x)} \\right] \\simeq \\frac{1}{L}\\sum_l \\log\\frac{p_\\theta(x|z_l)p(z_l)}{q_\\phi(z_l|x)}$,\n",
        "\n",
        "and, following [1], let $L=1$. \n",
        "\n",
        "Finally, package `tfp.distributions` could be used to calculate the (log) probability of different distributions. For instance, we could use `tfp.distributions.Bernoulli` to compute the likelihood of the pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ8LpygTOM4B"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def get_elbo(params, x, rng):\n",
        "    # the negative elbo should be a scalar value, i.e. averaged over the mini-batch.\n",
        "    (mu,sigma), z, bernoulli_theta = forward(params, x, rng)\n",
        "\n",
        "    # write code to compute the ELBO\n",
        "    return -elbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6CCP5aEMWC5"
      },
      "source": [
        "Now to the training and evaluation steps. Note that we have normalized the pixel values in order for them to fit the probabilistic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H32Pvk740Lb5"
      },
      "outputs": [],
      "source": [
        "def epoch_step(epoch, training_generator, opt_state):\n",
        "    # an rng, random number generator, is required for all stochastic operations\n",
        "    rng = random.PRNGKey(epoch)\n",
        "    nlls = []\n",
        "    for i, (x, y) in enumerate(tqdm(training_generator)):\n",
        "        x = jax.device_put(x) / 255.0\n",
        "\n",
        "        value, grads = jax.value_and_grad(get_elbo)(get_params(opt_state), x, rng)\n",
        "        opt_state = opt_update(i, grads, opt_state)\n",
        "\n",
        "        nlls.append(value)\n",
        "    return opt_state, np.mean(nlls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OSMK0inNWzI"
      },
      "source": [
        "Luckily, the complete test set fits in the memory, and so we can compute the average NLL on all images at once without taking the batch size into account. This might not always be possible, in which case you must be careful regarding how you average the NLL scores, since all batches needn't be of the same size. Failing to do so will make your NLL scores unfit for benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9shlWPR49iot"
      },
      "outputs": [],
      "source": [
        "def evaluate(opt_state, x, test_rng):\n",
        "    params = get_params(opt_state)\n",
        "    x = jax.device_put(x) / 255.0\n",
        "    nll = get_elbo(params, x, test_rng)\n",
        "    return nll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xl3a5C7UOAI"
      },
      "source": [
        "**Plot the NLL vs. epochs curves (train and test) and see that they're decaying.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUkouaNicpci"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "test_key = random.PRNGKey(10)\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    opt_state, nll_train = epoch_step(epoch, training_generator, opt_state)\n",
        "    nll_test = evaluate(opt_state, test_images, test_key)\n",
        "    print(f\"Train NLL: {nll_train}\")\n",
        "    print(\"Test NLL: \", nll_test, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch9ZyBPLVE5F"
      },
      "source": [
        "### Task 3: Experiments\n",
        "\n",
        "Let's put the trained VAE to use by visulizing the latent space and sampling synthetic data. Complete the following code snippets to do so.\n",
        "**Visualize the latent space. Use the labels $y$ to color the latent representations in a scatter plot.** \n",
        "\n",
        "Hint: Encode the training or test set, and use `plt.scatter`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3weOCVC7DCUb"
      },
      "outputs": [],
      "source": [
        "def show_latent_space():\n",
        "  encoder_params = get_params(opt_state)[0]\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF_qdMp_cu-7"
      },
      "source": [
        "Monitor the reconstruction quality of your model. Visualize a single input image together its reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5npepkIfEa_"
      },
      "outputs": [],
      "source": [
        "def reconstruct_an_image():\n",
        "  x = test_images[0]  # the image to be reconstructed\n",
        "  params = get_params(opt_state)\n",
        "  # call the forwad function and use its output to reconstruct the image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOhO21k6WjuN"
      },
      "source": [
        "**Generate synthetic data.** Hint: sample $z\\sim p(z)$ or \"walk\" in the latent space. Decode the $z$ you sample/visit and plot the decoded image. Think about what output you expect when you sample from 1) a Bernoulli distribution, or 2) regard the Bernoulli parameter as the sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3gKRLnZ_U5c"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def sample_image(decoder_params):\n",
        "    # feel free to add other args to the function\n",
        "    # either sample from the prior, or walk/do a grid search over z's\n",
        "    # then use decoder_params to decode the latent samples\n",
        "    return sampled_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwYA47KrNirt"
      },
      "source": [
        "## $\\beta$-VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0BdW4PyCr7M"
      },
      "source": [
        "The $\\beta$-VAE was proposed by [Higgins et al.](https://openreview.net/pdf?id=Sy2fzU9gl) in 2017 and is a straightforward but important extension of the VAE. In terms of architecture, $\\beta$-VAE and VAE are indistinguishable. In fact, the novelty in the work by Higgins et al. lies in a minor modification of the ELBO objective, the $\\beta$-objective\n",
        "\n",
        "$\\mathcal{L}_\\beta = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\beta \\mathbb{E}_{q_\\phi(z|x)}[\\log \\frac{q_\\phi(z|x)}{p(z)}]$,\n",
        "\n",
        "where $\\beta\\geq0$ is a hyperparameter weighting the KL divergence \n",
        "\n",
        "KL$(q_\\phi(z|x)\\Vert p(z)) = \\mathbb{E}_{q_\\phi(z|x)}[\\log \\frac{q_\\phi(z|x)}{p(z)}]$. Observe that we retrieve the standard ELBO when $\\beta=1$.\n",
        "\n",
        "This objective is useful for \n",
        "\n",
        "* emphasizing high-fidelity reconstructions *or* disentangling the latent space\n",
        "* obtaining an increased interpretability\n",
        "* doing KL warm-up.\n",
        "\n",
        "The effects of the $\\beta$-objective are mostly visible asymptotically, why we may not experience the full power of it here (e.g., [NVAE](https://arxiv.org/pdf/2007.03898.pdf) applies KL warm-up during the *first 25k iterations*). \n",
        "\n",
        "However, plenty of the state-of-the-art VAEs (in terms of density estimation) apply KL warm-up (e.g. NVAE, [VampPrior](https://arxiv.org/pdf/1705.07120.pdf), [LadderVAE](https://arxiv.org/pdf/1602.02282.pdf); see LadderVAE for a nice motivation of KL warm-up), so it is good to know. Also, aside from being very straightforward to implement, the $\\beta$-objective leads us to the following important note.\n",
        "\n",
        "**Important note**: as pointed out by [Hoffman et al.](http://bayesiandeeplearning.org/2017/papers/66.pdf), the $\\beta$-objective shown above cannot be considered an evidence lower-bound. Hence, test scores based on the $\\beta$-objective **should not be used for benchmarking against proper marginal log-likelihood estimates (such as the ELBO)**. If you are interested, try to reverse the derivations of the ELBO in [1] while including $\\beta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F49Zqa5kW8RL"
      },
      "source": [
        "### Task 4: $\\beta$-ELBO\n",
        "\n",
        "**Compute the $\\beta$-objective in the following function.**\n",
        "\n",
        "Hint: this should require minimal changes to the ```get_elbo```\n",
        "function above. Also, observe that\n",
        "\n",
        "\n",
        "\n",
        "$\\mathcal{L}_\\beta \\simeq \\frac{1}{L}\\sum_l \\log p_\\theta(x|z_l)-\\beta\\log\\frac{q_\\phi(z_l|x)}{p(z_l)}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hg7sWkcW7El"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def get_beta_elbo(params, x, rng, beta):\n",
        "    # the negative elbo should be a scalar value, i.e. averaged over the mini-batch.\n",
        "    (mu, sigma), z, bernoulli_theta = forward(params, x, rng)\n",
        "    # write code to compute the ELBO\n",
        "    return -beta_elbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIfPA1tWZmZc"
      },
      "source": [
        "Now we define ```epoch_step_beta``` that calls ```get_beta_elbo``` instead of ```get_elbo```, and so that it can take a $\\beta\\geq 0$ as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hhkfhf49xRsl"
      },
      "outputs": [],
      "source": [
        "def epoch_step_beta(epoch, training_generator, opt_state, beta):\n",
        "    # an rng, random number generator, is required for all stochastic operations\n",
        "    rng = random.PRNGKey(epoch)\n",
        "    nlls = []\n",
        "    for i, (x, y) in enumerate(tqdm(training_generator)):\n",
        "        x = jax.device_put(x) / 255\n",
        "        value, grads = jax.value_and_grad(get_beta_elbo)(get_params(opt_state),\n",
        "                                                         x, \n",
        "                                                         rng,\n",
        "                                                         beta)\n",
        "        opt_state = opt_update(i, \n",
        "                               grads, \n",
        "                               opt_state)\n",
        "        nlls.append(value)\n",
        "    return opt_state, np.mean(nlls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_tNCcGPdAEM"
      },
      "source": [
        "Again, importantly, we have used $\\beta=1$ as default in ```evaluate_beta```, which makes the evaluation scores comparable to those of the standard VAE that we trained above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r5NQhDOz3e5"
      },
      "outputs": [],
      "source": [
        "def evaluate_beta(opt_state, x, test_rng,beta=1):\n",
        "    params = get_params(opt_state)\n",
        "    x = jax.device_put(x) / 255\n",
        "    nll = get_beta_elbo(params,\n",
        "                        x, \n",
        "                        test_rng,beta)\n",
        "    return nll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcjreZ7qpi0g"
      },
      "source": [
        "Before retraining the VAE, or $\\beta$-VAE, we need to reinitialize the encoder/decoder nets and the optimizer above. This is done by simply running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ivjEgQZpemA"
      },
      "outputs": [],
      "source": [
        "beta = 100  # just an example for a big beta. Try with other values!\n",
        "latent_dimensions = 2\n",
        "\n",
        "# encoder defs\n",
        "encoder_init, encode = stax.serial(\n",
        "    Dense(300), Relu,\n",
        "    Dense(300), Relu,\n",
        "    FanOut(2),\n",
        "    stax.parallel(Dense(latent_dimensions), stax.serial(Dense(latent_dimensions), Softplus)),\n",
        ")\n",
        "\n",
        "# decoder defs\n",
        "decoder_init, decode = stax.serial(\n",
        "    Dense(300), Relu,\n",
        "    Dense(300), Relu,\n",
        "    Dense(28 * 28), Sigmoid\n",
        ")\n",
        "\n",
        "\n",
        "# initialize networks and get params\n",
        "enc_init_key, dec_init_key = random.split(random.PRNGKey(100))\n",
        "_, encoder_params = encoder_init(enc_init_key, input_shape=(-1, 28 * 28))\n",
        "_, decoder_params = decoder_init(dec_init_key,\n",
        "                                 input_shape=\n",
        "                                 (-1, latent_dimensions))\n",
        "params = encoder_params, decoder_params\n",
        "\n",
        "# initialize optimizer\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size=0.001)\n",
        "opt_state = opt_init(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjC-cG2Qb_1J"
      },
      "source": [
        "Rerun the experiments section below. Any difference? If you train for a few couple of epochs, then we shouldn't expect huge variations. But perhaps setting $\\beta$ to something very big will affect the distribution of $z$'s in the latent space? Does setting $\\beta=0$ affect the fidelity of the reconstructed image? Why would we, at least asymptotically, expect these effects?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "munYoilMqG1C"
      },
      "source": [
        "### Task 4: $\\beta$-Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7CWem3Gqb80"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "train_key,test_key = random.split(random.PRNGKey(10))\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    opt_state, nll_train = epoch_step_beta(epoch,\n",
        "                                           training_generator, \n",
        "                                           opt_state,\n",
        "                                           beta)\n",
        "    nll_test = evaluate_beta(opt_state, \n",
        "                             test_images,\n",
        "                             test_key)\n",
        "    print(\"Train NLL: \", nll_train)\n",
        "    print(\"Test NLL: \", nll_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZJZJLeLzuU9"
      },
      "source": [
        "What has changed in the latent space? Try playing with a few values of $\\beta$ and explain what affects the change in the latent space?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_mRUuW6sffV"
      },
      "outputs": [],
      "source": [
        "# visualize the latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJLhpfdrz8lg"
      },
      "source": [
        "What about the reconstruction quality of your model? Has it improved or degraded?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn7eNJFUOsQ1"
      },
      "outputs": [],
      "source": [
        "# reconstruct an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEFmyJ1u0Lnf"
      },
      "source": [
        "Sample from the latent space and decode. Do you see any difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDPJW6jjsgaX"
      },
      "outputs": [],
      "source": [
        "# sample a few images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-bQqSYGN4oH"
      },
      "source": [
        "## Importance Weigthed Auto-Encoder (IWAE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkiy7FkJeigq"
      },
      "source": [
        "Similarily as for $\\beta$-VAE, the distinguishing factor between the VAE and [IWAE](https://arxiv.org/abs/1509.00519) is the alternative objective function, the importance weighted ELBO (IWELBO):\n",
        "\n",
        "$\\mathcal{L}_{K} = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{1}{K}\\sum_k\\frac{p_\\theta(x|z_k)p(z_k)}{q_\\phi(z_k|x)}\\right]$.\n",
        "\n",
        "When computing $\\mathcal{L}_{K}$ in contrast to the ELBO, we sample $K$ latent representations instead of one. Although this might seem like a simple extension, the IWELBO has huge implications on the field of VAEs, *as well as* the VI literature in general (see e.g. [Domke and Sheldon](https://arxiv.org/pdf/1808.09034.pdf)). Specifically (n.b., *not an exhaustive list*),\n",
        "\n",
        "* it is a proper lower-bound on the marginal log-likelihood.\n",
        "* it is an effective way of improving the NLL test scores (used by state-of-the-art VAEs when reporting NLL scores, including those mentioned above).\n",
        "\n",
        "The former bullet implies that we *can* use IWELBO to benchmark our VAE against other VAEs, in contrast to the $\\beta$-objective. The latter bullet implies that we probably *should* use it when benchmarking. Because, accoring to  Theorem 1 in the IWAE paper, the marginal log-likelihood estimates are montonically increasing with $K$, meaning that\n",
        "\n",
        "$\\mathcal{L}_{K}\\geq \\mathcal{L}_{K-1}\\geq \\mathcal{L}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHkXKbG1iPSK"
      },
      "source": [
        "### Task 5: IWELBO\n",
        "\n",
        "Modify the following functions such that $K$ samples are drawn instead of 1. Then compute the IWELBO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js95gDCxbpz2"
      },
      "outputs": [],
      "source": [
        "def sample_K(rng, mu, std, K):\n",
        "  # use the reparameterization trick and sample K latent variables rather than 1\n",
        "  return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFTXfZL4cIc8"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnames=['K'])\n",
        "def forward_K(params, x, rng, K):\n",
        "    encoder_params, decoder_params = params\n",
        "    phi = encode(encoder_params, inputs=x)\n",
        "    z = sample_K(rng,\n",
        "            mu=phi[0], \n",
        "            std=phi[1],\n",
        "            K=K)\n",
        "    bernoulli_theta = decode(decoder_params, z)\n",
        "    return phi, z, bernoulli_theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbZKMRXCgFGn"
      },
      "outputs": [],
      "source": [
        "def get_iw_elbo(params, x, rng, K):\n",
        "    # the negative elbo should be a scalar value, i.e. averaged over the mini-batch.\n",
        "    (mu, sigma), z, bernoulli_theta = forward_K(params, x, rng, K)\n",
        "    # hint: use the logsumexp() operation over K (available in the jax library) \n",
        "    # and subtract with logK to do the average inside the log.\n",
        "    return -iw_elbo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahUpB2H_c96Q"
      },
      "outputs": [],
      "source": [
        "def epoch_step_K(epoch, training_generator, opt_state,beta):\n",
        "    # an rng, random number generator, is required for all stochastic operations\n",
        "    rng = random.PRNGKey(epoch)\n",
        "    nlls = []\n",
        "    for i, (x, y) in enumerate(tqdm(training_generator)):\n",
        "        x = jax.device_put(x) / 255\n",
        "        value, grads = jax.value_and_grad(get_iw_elbo)(get_params(opt_state),\n",
        "                                                         x, \n",
        "                                                         rng,\n",
        "                                                         K)\n",
        "        opt_state = opt_update(i, \n",
        "                               grads, \n",
        "                               opt_state)\n",
        "        nlls.append(value)\n",
        "    return opt_state, np.mean(nlls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wylN4mLd6SVf"
      },
      "outputs": [],
      "source": [
        "def evaluate_K(opt_state, x, test_rng, K):\n",
        "    params = get_params(opt_state)\n",
        "    x = jax.device_put(x) / 255.0\n",
        "    params, x = jax.lax.stop_gradient(params), jax.lax.stop_gradient(x)\n",
        "    nll = get_iw_elbo(params, x, test_rng, K)\n",
        "    return nll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j0lyDE0gwRP"
      },
      "source": [
        "Again, we need to re-initialize the network and optimizer states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6NBnQYkcOdB"
      },
      "outputs": [],
      "source": [
        "K = 4  # try with different values ranging from 1 to 10\n",
        "latent_dimensions = 2\n",
        "\n",
        "# encoder defs\n",
        "encoder_init, encode = stax.serial(\n",
        "    Dense(300), Relu,\n",
        "    Dense(300), Relu,\n",
        "    FanOut(2),\n",
        "    stax.parallel(Dense(latent_dimensions), stax.serial(Dense(latent_dimensions), Softplus)),\n",
        ")\n",
        "\n",
        "# decoder defs\n",
        "decoder_init, decode = stax.serial(\n",
        "    Dense(300), Relu,\n",
        "    Dense(300), Relu,\n",
        "    Dense(28 * 28), Sigmoid\n",
        ")\n",
        "\n",
        "\n",
        "# initialize networks and get params\n",
        "enc_init_key, dec_init_key = random.split(random.PRNGKey(100))\n",
        "_, encoder_params = encoder_init(enc_init_key, input_shape=(-1, 28 * 28))\n",
        "_, decoder_params = decoder_init(dec_init_key,\n",
        "                                 input_shape=\n",
        "                                 (-1, latent_dimensions))\n",
        "params = encoder_params, decoder_params\n",
        "\n",
        "# initialize optimizer\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size=0.0001)  # this time we use a smaller learning rate due to numerical stability\n",
        "opt_state = opt_init(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PpatzKTcRFl"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "test_key = random.PRNGKey(10)\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    opt_state, nll_train = epoch_step_K(epoch, \n",
        "                                      training_generator, \n",
        "                                      opt_state,\n",
        "                                      K)\n",
        "    nll_test = evaluate_K(opt_state, test_images, test_key, K)\n",
        "    print(\"Train NLL: \", nll_train)\n",
        "    print(\"Test NLL: \", nll_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7byOMJnbxiR1"
      },
      "source": [
        "Plot your NLL test scores as a function of $K$ for $K \\in \\{1,\\dots,10\\}$, and see if there is any improvement in NLL as K increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_2HGa-dfZOR"
      },
      "outputs": [],
      "source": [
        "def plot_test_nll_vs_k():\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHLkHIlbvTbe"
      },
      "source": [
        "Finally, it is valuable to know, how long did it take you to finish this practical?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sStHLxRtx4oE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
